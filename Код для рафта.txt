4569-412:31 16.04.20246049050645609405696456946-.546049569..43584834508345frrrjfdb9439044
6t57568495884395869405694-8694562459045689548693495805000005555
0101010101010101010-466-9349484-01010101010-3.
/raft 456345363636010101010.we.com
from urllib.request import urlopen         # из модуля urllib импортируем функцию urlopen

u = urlopen("http://python.org")    # открываем URL на чтение
words = {}                          # связываем имя words с пустым словарём
                                    # (словарь — неупорядоченный [[ассоциативный массив]])
for line in u:                      # читаем u по строкам
    line =line.decode("utf-8")      # преобразуем байт-строку в строку
    line = line.strip(" \n")       # отбрасываем начальные и конечные пробелы
    for word in line.split(" "):    # режем каждую строку на слова, ограниченные пробелами
        try:                            # блок обработки исключений
            words[word] += 1            # пытаемся увеличить words[word] на единицу
        except KeyError:                # если не получилось (раньше words[word] не было)
            words[word] = 1             # присваиваем единицу
            
# теперь словарь words содержит частоту встречаемости каждого слова.
# Например, words может содержать {"яблоко":5, "апельсин": 12, "груша": 8}

pairs = words.items()               # делаем из словаря список пар
                                    # pairs == [("яблоко",5), ("апельсин",12), ("груша",8)]
A= sorted (pairs, key=lambda x: x[1], reverse=True)  # сортируем по убыванию второго элемента пары

for p in A[:10]:                # печатаем первые 10 элементов списка
    print(p[0], p[1])from urllib.request import urlopen         # из модуля urllib импортируем функцию urlopen

u = urlopen("http://python.org")    # открываем URL на чтение
words = {}                          # связываем имя words с пустым словарём
                                    # (словарь — неупорядоченный [[ассоциативный массив]])
for line in u:                      # читаем u по строкам
    line =line.decode("utf-8")      # преобразуем байт-строку в строку
    line = line.strip(" \n")       # отбрасываем начальные и конечные пробелы
    for word in line.split(" "):    # режем каждую строку на слова, ограниченные пробелами
        try:                            # блок обработки исключений
            words[word] += 1            # пытаемся увеличить words[word] на единицу
        except KeyError:                # если не получилось (раньше words[word] не было)
            words[word] = 1             # присваиваем единицу
            
# теперь словарь words содержит частоту встречаемости каждого слова.
# Например, words может содержать {"яблоко":5, "апельсин": 12, "груша": 8}

pairs = words.items()               # делаем из словаря список пар
                                    # pairs == [("яблоко",5), ("апельсин",12), ("груша",8)]
A= sorted (pairs, key=lambda x: x[1], reverse=True)  # сортируем по убыванию второго элемента пары

for p in A[:10]:                # печатаем первые 10 элементов списка
    print(p[0], p[1])